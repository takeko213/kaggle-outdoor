{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "responsible-balance",
   "metadata": {},
   "source": [
    "# distance_pred_v001\n",
    "speed予測の検討（センサのみ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "surface-extent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import library\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2, venn2_circles\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import pathlib\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import itertools\n",
    "import lightgbm as lgb\n",
    "from optuna.integration import lightgbm as optuna_lgb\n",
    "import simdkalman\n",
    "import optuna\n",
    "import pyproj\n",
    "from pyproj import Proj, transform\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, confusion_matrix, accuracy_score\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "worst-disposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb_path\n",
    "\n",
    "def get_nb_name():\n",
    "    nb_path = ipynb_path.get()\n",
    "    nb_name = nb_path.rsplit('/',1)[1].replace('.ipynb','')\n",
    "    return nb_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "divine-headset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory setting\n",
    "nb_name = get_nb_name()\n",
    "INPUT = '../input/google-smartphone-decimeter-challenge'\n",
    "OUTPUT = '../output/prep/' + nb_name\n",
    "os.makedirs(OUTPUT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-burns",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ideal-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_score(df, gt):\n",
    "    gt = gt.rename(columns={'latDeg':'latDeg_gt', 'lngDeg':'lngDeg_gt'})\n",
    "    df = df.merge(gt, on=['collectionName', 'phoneName', 'millisSinceGpsEpoch'], how='inner')\n",
    "    # calc_distance_error\n",
    "    df['err'] = calc_haversine(df['latDeg_gt'], df['lngDeg_gt'], df['latDeg'], df['lngDeg'])\n",
    "    # calc_evaluate_score\n",
    "    df['phone'] = df['collectionName'] + '_' + df['phoneName']\n",
    "    res = df.groupby('phone')['err'].agg([percentile50, percentile95])\n",
    "    res['p50_p90_mean'] = (res['percentile50'] + res['percentile95']) / 2 \n",
    "    score = res['p50_p90_mean'].mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "legislative-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculates the great circle distance between two points\n",
    "    on the earth. Inputs are array-like and specified in decimal degrees.\n",
    "    \"\"\"\n",
    "    RADIUS = 6_367_000\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + \\\n",
    "        np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    dist = 2 * RADIUS * np.arcsin(a**0.5)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecological-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_trafic(df, center, zoom=9):\n",
    "    fig = px.scatter_mapbox(df,\n",
    "                            \n",
    "                            # Here, plotly gets, (x,y) coordinates\n",
    "                            lat=\"latDeg\",\n",
    "                            lon=\"lngDeg\",\n",
    "                            \n",
    "                            #Here, plotly detects color of series\n",
    "                            color=\"phoneName\",\n",
    "                            labels=\"phoneName\",\n",
    "                            \n",
    "                            zoom=zoom,\n",
    "                            center=center,\n",
    "                            height=600,\n",
    "                            width=800)\n",
    "    fig.update_layout(mapbox_style='stamen-terrain')\n",
    "    fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "    fig.update_layout(title_text=\"GPS trafic\")\n",
    "    fig.show()\n",
    "    \n",
    "def visualize_collection(df, collection):\n",
    "    target_df = df[df['collectionName']==collection].copy()\n",
    "    lat_center = target_df['latDeg'].mean()\n",
    "    lng_center = target_df['lngDeg'].mean()\n",
    "    center = {\"lat\":lat_center, \"lon\":lng_center}\n",
    "    \n",
    "    visualize_trafic(target_df, center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "grave-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground_truth\n",
    "def get_ground_truth():\n",
    "    p = pathlib.Path(INPUT)\n",
    "    gt_files = list(p.glob('train/*/*/ground_truth.csv'))\n",
    "\n",
    "    gts = []\n",
    "    for gt_file in gt_files:\n",
    "        gts.append(pd.read_csv(gt_file))\n",
    "    ground_truth = pd.concat(gts)\n",
    "\n",
    "    return ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "derived-buffalo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile50(x):\n",
    "    return np.percentile(x, 50)\n",
    "def percentile95(x):\n",
    "    return np.percentile(x, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "average-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_result:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.gt = get_ground_truth()\n",
    "        self.bl = pd.read_csv(INPUT + '/' + 'baseline_locations_train.csv')\n",
    "        \n",
    "        self.gt = self.gt.rename(columns={'latDeg':'latDeg_gt', 'lngDeg':'lngDeg_gt'})\n",
    "        self.df = self.df.merge(self.gt, on=['collectionName', 'phoneName', 'millisSinceGpsEpoch'], how='inner')\n",
    "        self.df['phone'] = self.df['collectionName'] + '_' + self.df['phoneName']\n",
    "        self.df['err'] =  calc_haversine(self.df['latDeg_gt'], self.df['lngDeg_gt'], self.df['latDeg'], self.df['lngDeg'])\n",
    "        \n",
    "        self.phone_res = self.calc_err('phone')\n",
    "        self.clc_res = self.calc_err('collectionName')\n",
    "        self.phonename_res = self.calc_err('phoneName')\n",
    "        \n",
    "    def calc_err(self, by):\n",
    "        res = self.df.groupby(by)['err'].agg([percentile50, percentile95])\n",
    "        res['p50_p90_mean'] = (res['percentile50'] + res['percentile95']) / 2\n",
    "        return res\n",
    "    \n",
    "    @property\n",
    "    def score(self):\n",
    "        return self.phone_res['p50_p90_mean'].mean()\n",
    "    @property\n",
    "    def raw_data(self):\n",
    "        return self.df\n",
    "    @property\n",
    "    def err(self):\n",
    "        return self.phone_res\n",
    "    @property\n",
    "    def collection_err(self):\n",
    "        return self.clc_res\n",
    "    @property\n",
    "    def phonename_err(self):\n",
    "        return self.phonename_res\n",
    "    \n",
    "    def viz_map(self, collection, show_gt=True, show_bl=True):\n",
    "        tmp = self.df[self.df['collectionName']==collection][['collectionName', 'phoneName', 'latDeg', 'lngDeg']]\n",
    "        tmp2 = self.df[self.df['collectionName']==collection][['collectionName', 'phoneName', 'latDeg_gt', 'lngDeg_gt']]\n",
    "        tmp2 = tmp2.rename(columns={'latDeg_gt':'latDeg', 'lngDeg_gt':'lngDeg'})\n",
    "        tmp2['phoneName'] = tmp2['phoneName'] + '_GT'\n",
    "        tmp3 = self.bl[self.bl['collectionName']==collection][['collectionName', 'phoneName', 'latDeg', 'lngDeg']]\n",
    "        tmp3['phoneName'] = tmp3['phoneName'] + '_BL'\n",
    "        \n",
    "        if show_gt:\n",
    "            tmp = tmp.append(tmp2)\n",
    "        if show_bl:\n",
    "            tmp = tmp.append(tmp3)\n",
    "        visualize_collection(tmp, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "elementary-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    base_train = pd.read_csv(INPUT + '/' + 'baseline_locations_train.csv')\n",
    "    base_test = pd.read_csv(INPUT + '/' + 'baseline_locations_test.csv')\n",
    "    sample_sub = pd.read_csv(INPUT + '/' + 'sample_submission.csv')\n",
    "    ground_truth = get_ground_truth()\n",
    "    return base_train, base_test, sample_sub, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "amended-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    for c,i in itertools.product(['latDeg', 'lngDeg', 'heightAboveWgs84EllipsoidM'], [1,2,3,4,5,-1,-2,-3,-4,-5]):\n",
    "        col = c+ '_s' + str(i)\n",
    "        df[col] = df[c].shift(i)\n",
    "        df[col+'_diff'] = df[c] - df[col]\n",
    "        df.loc[df['phone']!=df['phone'].shift(i), [col, col+'_diff']] = np.nan\n",
    "    \n",
    "    for c in ['latDeg', 'lngDeg', 'heightAboveWgs84EllipsoidM']:\n",
    "        df[c+'_s1_diff_sum'] = df[c+'_s1_diff'].fillna(0) + df[c+'_s-1_diff'].fillna(0)\n",
    "        df[c+'_s2_diff_sum'] = df[c+'_s1_diff_sum'] + df[c+'_s2_diff'].fillna(0) + df[c+'_s-2_diff'].fillna(0)\n",
    "        df[c+'_s3_diff_sum'] = df[c+'_s2_diff_sum'] + df[c+'_s3_diff'].fillna(0) + df[c+'_s-3_diff'].fillna(0)\n",
    "        df[c+'_s4_diff_sum'] = df[c+'_s3_diff_sum'] + df[c+'_s4_diff'].fillna(0) + df[c+'_s-4_diff'].fillna(0)\n",
    "        df[c+'_s5_diff_sum'] = df[c+'_s4_diff_sum'] + df[c+'_s5_diff'].fillna(0) + df[c+'_s-5_diff'].fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "minor-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sensor_features(df, accel, gyro, mag, ori):\n",
    "    # phoneを追加\n",
    "    df['phone'] = df['collectionName'] + '_' + df['phoneName']\n",
    "    accel['phone'] = accel['collectionName'] + '_' + accel['phoneName']\n",
    "    gyro['phone'] = gyro['collectionName'] + '_' + gyro['phoneName']\n",
    "    mag['phone'] = mag['collectionName'] + '_' + mag['phoneName']\n",
    "    ori['phone'] = ori['collectionName'] + '_' + ori['phoneName']\n",
    "    \n",
    "     # 一定の値しか入っていないphoneを除外しておく\n",
    "    ori = ori[~ori['phone'].isin(['2021-04-29-US-MTV-1_SamsungS20Ultra', '2021-04-28-US-MTV-1_SamsungS20Ultra', '2021-04-28-US-SJC-1_SamsungS20Ultra', '2021-04-29-US-SJC-2_SamsungS20Ultra',\n",
    "                                 '2021-04-28-US-MTV-2_SamsungS20Ultra', '2021-04-29-US-SJC-3_SamsungS20Ultra', '2021-04-29-US-MTV-2_SamsungS20Ultra'])]\n",
    "    \n",
    "    \n",
    "    # phonenameをラベルエンコーディング\n",
    "    phoneName_map = {'Pixel4':1, 'Pixel4XLModded':2, 'Pixel4XL':3, 'Mi8':4, 'Pixel4Modded':5, 'Pixel5':6, 'SamsungS20Ultra':7}\n",
    "    df['phoneName_le'] = df['phoneName'].map(phoneName_map)\n",
    "    \n",
    "    # utc -> gps\n",
    "    accel['millisSinceGpsEpoch'] = accel['utcTimeMillis'] - 315964800000 + 18000\n",
    "    gyro['millisSinceGpsEpoch'] = gyro['utcTimeMillis'] - 315964800000 + 18000\n",
    "    mag['millisSinceGpsEpoch'] = mag['utcTimeMillis'] - 315964800000 + 18000\n",
    "    ori['millisSinceGpsEpoch'] = ori['utcTimeMillis'] - 315964800000 + 18000\n",
    "    \n",
    "    # resampling追加\n",
    "    df['secondSinceGpsEpoch'] = df['millisSinceGpsEpoch'] // 1000\n",
    "    accel['secondSinceGpsEpoch'] = accel['millisSinceGpsEpoch'] // 1000\n",
    "    gyro['secondSinceGpsEpoch'] = gyro['millisSinceGpsEpoch'] // 1000\n",
    "    mag['secondSinceGpsEpoch'] = mag['millisSinceGpsEpoch'] // 1000\n",
    "    ori['secondSinceGpsEpoch'] = ori['millisSinceGpsEpoch'] // 1000\n",
    "    \n",
    "    # clipping\n",
    "    accel[['UncalAccelXMps2', 'UncalAccelYMps2', 'UncalAccelZMps2']] = accel.groupby('phone')['UncalAccelXMps2', 'UncalAccelYMps2', 'UncalAccelZMps2'].transform(lambda x: x.clip(x.quantile(0.001), x.quantile(0.999)))\n",
    "    gyro[['UncalGyroXRadPerSec', 'UncalGyroYRadPerSec', 'UncalGyroZRadPerSec']] = gyro.groupby('phone')['UncalGyroXRadPerSec', 'UncalGyroYRadPerSec', 'UncalGyroZRadPerSec'].transform(lambda x: x.clip(x.quantile(0.001), x.quantile(0.999)))\n",
    "    mag[['UncalMagXMicroT', 'UncalMagYMicroT', 'UncalMagZMicroT']] = mag.groupby('phone')['UncalMagXMicroT', 'UncalMagYMicroT', 'UncalMagZMicroT'].transform(lambda x: x.clip(x.quantile(0.001), x.quantile(0.999)))\n",
    "    \n",
    "    accel = accel.groupby(['phone', 'secondSinceGpsEpoch'])['UncalAccelXMps2', 'UncalAccelYMps2', 'UncalAccelZMps2'].agg(['mean', 'std']).reset_index()\n",
    "    accel.columns = ['phone', 'secondSinceGpsEpoch', 'UncalAccelXMps2_mean', 'UncalAccelXMps2_std', 'UncalAccelYMps2_mean', 'UncalAccelYMps2_std', 'UncalAccelZMps2_mean', 'UncalAccelZMps2_std']\n",
    "    gyro = gyro.groupby(['phone', 'secondSinceGpsEpoch'])['UncalGyroXRadPerSec', 'UncalGyroYRadPerSec', 'UncalGyroZRadPerSec'].agg(['mean', 'std']).reset_index()\n",
    "    gyro.columns = ['phone', 'secondSinceGpsEpoch', 'UncalGyroXRadPerSec_mean', 'UncalGyroXRadPerSec_std', 'UncalGyroYRadPerSec_mean', 'UncalGyroYRadPerSec_std', 'UncalGyroZRadPerSec_mean', 'UncalGyroZRadPerSec_std' ]\n",
    "    mag = mag.groupby(['phone', 'secondSinceGpsEpoch'])['UncalMagXMicroT', 'UncalMagYMicroT', 'UncalMagZMicroT'].agg(['mean', 'std']).reset_index()\n",
    "    mag.columns = ['phone', 'secondSinceGpsEpoch', 'UncalMagXMicroT_mean', 'UncalMagXMicroT_std', 'UncalMagYMicroT_mean', 'UncalMagYMicroT_std', 'UncalMagZMicroT_mean', 'UncalMagZMicroT_std']\n",
    "    ori = ori.groupby(['phone', 'secondSinceGpsEpoch'])['yawDeg', 'rollDeg', 'pitchDeg'].agg(['mean', 'std']).reset_index()\n",
    "    ori.columns = ['phone', 'secondSinceGpsEpoch', 'yawDeg_mean', 'yawDeg_std', 'rollDeg_mean', 'rollDeg_std', 'pitchDeg_mean', 'pitchDeg_std']\n",
    "    \n",
    "    \n",
    "    # shift特徴量\n",
    "    for c, i in itertools.product(['UncalAccelXMps2_mean', 'UncalAccelXMps2_std', 'UncalAccelYMps2_mean', 'UncalAccelYMps2_std', 'UncalAccelZMps2_mean', 'UncalAccelZMps2_std'], [1,2,3,4,5-1,-2,-3,-4,-5]):\n",
    "        col = c+ '_s' + str(i)\n",
    "        accel[col] = accel[c].shift(i)\n",
    "        accel[col+'_diff'] = accel[c] - accel[col]\n",
    "        accel.loc[accel['phone']!=accel['phone'].shift(i), [col, col+'_diff']] = np.nan\n",
    "    for c, i in itertools.product(['UncalGyroXRadPerSec_mean', 'UncalGyroXRadPerSec_std', 'UncalGyroYRadPerSec_mean', 'UncalGyroYRadPerSec_std', 'UncalGyroZRadPerSec_mean', 'UncalGyroZRadPerSec_std'], [1,2,3,4,5-1,-2,-3,-4,-5]):\n",
    "        col = c+ '_s' + str(i)\n",
    "        gyro[col] = gyro[c].shift(i)\n",
    "        gyro[col+'_diff'] = gyro[c] - gyro[col]\n",
    "        gyro.loc[gyro['phone']!=gyro['phone'].shift(i), [col, col+'_diff']] = np.nan\n",
    "    for c, i in itertools.product(['UncalMagXMicroT_mean', 'UncalMagXMicroT_std', 'UncalMagYMicroT_mean', 'UncalMagYMicroT_std', 'UncalMagZMicroT_mean', 'UncalMagZMicroT_std'], [1,2,3,4,5-1,-2,-3,-4,-5]):\n",
    "        col = c+ '_s' + str(i)\n",
    "        mag[col] = mag[c].shift(i)\n",
    "        mag[col+'_diff'] = mag[c] - mag[col]\n",
    "        mag.loc[mag['phone']!=mag['phone'].shift(i), [col, col+'_diff']] = np.nan\n",
    "    for c, i in itertools.product(['yawDeg_mean', 'yawDeg_std', 'rollDeg_mean', 'rollDeg_std', 'pitchDeg_mean', 'pitchDeg_std'], [1,2,3,-1,-2,-3]):\n",
    "        col = c+ '_s' + str(i)\n",
    "        ori[col] = ori[c].shift(i)\n",
    "        ori[col+'_diff'] = ori[c] - ori[col]\n",
    "        ori.loc[ori['phone']!=ori['phone'].shift(i), [col, col+'_diff']] = np.nan        \n",
    "    \n",
    "    df = df.merge(accel, on=['phone', 'secondSinceGpsEpoch'], how='left')\n",
    "    df = df.merge(gyro, on=['phone', 'secondSinceGpsEpoch'], how='left')\n",
    "    df = df.merge(mag, on=['phone', 'secondSinceGpsEpoch'], how='left')\n",
    "    df = df.merge(ori, on=['phone', 'secondSinceGpsEpoch'], how='left')\n",
    "    \n",
    "    df.drop(['secondSinceGpsEpoch'], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sound-easter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:32: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:33: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:34: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:36: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:38: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:40: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:42: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:32: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:33: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:34: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:36: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:38: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:40: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:42: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n"
     ]
    }
   ],
   "source": [
    "train, test, sub, gt = get_data()\n",
    "accel_train = pd.read_csv(INPUT + '/prep/gnss/train/UncalAccel.csv')\n",
    "gyro_train = pd.read_csv(INPUT + '/prep/gnss/train/UncalGyro.csv')\n",
    "mag_train = pd.read_csv(INPUT + '/prep/gnss/train/UncalMag.csv')\n",
    "ori_train = pd.read_csv(INPUT + '/prep/gnss/train/OrientationDeg.csv')\n",
    "accel_test = pd.read_csv(INPUT + '/prep/gnss/test/UncalAccel.csv')\n",
    "gyro_test = pd.read_csv(INPUT + '/prep/gnss/test/UncalGyro.csv')\n",
    "mag_test = pd.read_csv(INPUT + '/prep/gnss/test/UncalMag.csv')\n",
    "ori_test = pd.read_csv(INPUT + '/prep/gnss/test/OrientationDeg.csv')\n",
    "\n",
    "train = train.merge(gt[['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'speedMps']], on=['collectionName', 'phoneName', 'millisSinceGpsEpoch'], how='inner')\n",
    "train = add_sensor_features(train, accel_train, gyro_train, mag_train, ori_train)\n",
    "test = add_sensor_features(test, accel_test, gyro_test, mag_test, ori_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "numerical-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "target1 = 'speedMps'\n",
    "not_use_cols = ['speedMps', 'courseDegree', 'collectionName', 'phoneName', 'phone', 'millisSinceGpsEpoch', 'latDeg', 'lngDeg', 'heightAboveWgs84EllipsoidM',\n",
    "                'latDeg_s1', 'latDeg_s2', 'latDeg_s3', 'latDeg_s-1', 'latDeg_s-2',\n",
    "                'latDeg_s-3', 'lngDeg_s1', 'lngDeg_s2', 'lngDeg_s3', 'lngDeg_s-1',\n",
    "                'lngDeg_s-2', 'lngDeg_s-3', 'heightAboveWgs84EllipsoidM_s1',\n",
    "                'heightAboveWgs84EllipsoidM_s2', 'heightAboveWgs84EllipsoidM_s3',\n",
    "                'heightAboveWgs84EllipsoidM_s-1', 'heightAboveWgs84EllipsoidM_s-2',\n",
    "                'heightAboveWgs84EllipsoidM_s-3']\n",
    "\n",
    "features = [c for c in train.columns if c not in not_use_cols]\n",
    "\n",
    "opt_params = {'objective': 'regression',\n",
    "              'metrics': 'rmse',\n",
    "              'learning_rate': 0.1, \n",
    "              'seed': 42, \n",
    "              'feature_pre_filter': False, \n",
    "              'lambda_l1': 5.430530747001109e-05, \n",
    "              'lambda_l2': 3.4066721259729875, \n",
    "              'num_leaves': 136, \n",
    "              'feature_fraction': 0.8999999999999999, \n",
    "              'bagging_fraction': 1.0, \n",
    "              'bagging_freq': 0, \n",
    "              'min_child_samples': 20, \n",
    "              'num_iterations': 20000, \n",
    "              'early_stopping_round': 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-north",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid :  2020-05-14-US-MTV-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:153: UserWarning: Found `early_stopping_round` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.838082 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 97928\n",
      "[LightGBM] [Info] Number of data points in the train set: 127856, number of used features: 385\n",
      "[LightGBM] [Info] Start training from score 18.089451\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 5.2264\tvalid_1's rmse: 3.95106\n",
      "Early stopping, best iteration is:\n",
      "[19]\ttraining's rmse: 5.84738\tvalid_1's rmse: 3.78464\n",
      "valid :  2020-05-14-US-MTV-2\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.951274 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 97928\n",
      "[LightGBM] [Info] Number of data points in the train set: 128995, number of used features: 385\n",
      "[LightGBM] [Info] Start training from score 18.082400\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 5.20993\tvalid_1's rmse: 4.08362\n"
     ]
    }
   ],
   "source": [
    "collections = train['collectionName'].unique()\n",
    "\n",
    "oof1 = pd.DataFrame()\n",
    "imp1 = pd.DataFrame()\n",
    "test_preds = np.zeros(len(test))\n",
    "n = len(collections)\n",
    "\n",
    "for collection in collections:\n",
    "    print('valid : ', collection)\n",
    "    tr_idx = train[train['collectionName']!=collection].index\n",
    "    vl_idx = train[train['collectionName']==collection].index\n",
    "    tr_x, tr_y = train[features].iloc[tr_idx], train[target1].iloc[tr_idx]\n",
    "    vl_x, vl_y = train[features].iloc[vl_idx], train[target1].iloc[vl_idx]\n",
    "    tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "    vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "    \n",
    "    model = lgb.train(opt_params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                      num_boost_round=20000, early_stopping_rounds=100, verbose_eval=100)\n",
    "    vl_pred = model.predict(vl_x, num_iteration=model.best_iteration)\n",
    "    \n",
    "    oof_tmp = train.iloc[vl_idx].copy()\n",
    "    oof_tmp['pred'] = vl_pred\n",
    "    oof1 = oof1.append(oof_tmp)\n",
    "    \n",
    "    imp_tmp = pd.DataFrame()\n",
    "    imp_tmp['feature'] = model.feature_name()\n",
    "    imp_tmp['importance'] = model.feature_importance()\n",
    "    imp_tmp['valid_collection'] = collection\n",
    "    imp1 = imp1.append(imp_tmp)\n",
    "    \n",
    "    pred = model.predict(test[features], num_iteration=model.best_iteration)\n",
    "    test_preds += pred / n\n",
    "test['pred'] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-segment",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(oof1['speedMps'], oof1['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_mean = imp1.groupby('feature').mean().reset_index()\n",
    "plt.figure(figsize=(10, 50))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=imp_mean.sort_values(by=\"importance\", ascending=False))\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof1[['collectionName', 'phoneName', 'phone', 'millisSinceGpsEpoch', 'pred']].to_csv(OUTPUT + '/train_distance_pred.csv', index=False)\n",
    "test[['collectionName', 'phoneName', 'phone', 'millisSinceGpsEpoch', 'pred']].to_csv(OUTPUT + '/test_distance_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print('total : ', np.sqrt(mean_squared_error(oof1['speedMps'], oof1['pred'])))\n",
    "phones = oof1['phone'].unique()\n",
    "for phone in phones:\n",
    "    tmp = oof1[oof1['phone']==phone].copy()\n",
    "    print(phone + ' : ', np.sqrt(mean_squared_error(tmp['speedMps'], tmp['pred'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-numbers",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
